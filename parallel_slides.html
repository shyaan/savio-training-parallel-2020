<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="April 21, 2020" />
  <title>Savio parallel processing training</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Savio parallel processing training</h1>
  <p class="author">
April 21, 2020
  </p>
  <p class="date">Nicolas Chan, Wei Feinstein, Christopher Hann-Soden, Chris Paciorek</p>
</div>
<div id="introduction" class="slide section level1">
<h1>Introduction</h1>
<p>We’ll do this mostly as a demonstration. We encourage you to login to your account and try out the various examples yourself as we go through them.</p>
<p>Much of this material is based on the extensive Savio documention we have prepared and continue to prepare, available at our new documentation site: <a href="https://docs-research-it.berkeley.edu/services/high-performance-computing">https://docs-research-it.berkeley.edu/services/high-performance-computing</a> as well as our old site: <a href="http://research-it.berkeley.edu/services/high-performance-computing">http://research-it.berkeley.edu/services/high-performance-computing</a>.</p>
<p>The materials for this tutorial are available using git at the short URL (<a href="https://tinyurl.com/brc-apr20">https://tinyurl.com/brc-apr20</a>), the GitHub URL (<a href="https://github.com/ucb-rit/savio-training-parallel-2020">https://github.com/ucb-rit/savio-training-parallel-2020</a>), or simply as a <a href="https://github.com/ucb-rit/savio-training-parallel-2020/archive/master.zip">zip file</a>.</p>
</div>
<div id="outline" class="slide section level1">
<h1>Outline</h1>
<ul>
<li>Introduction (Chris)
<ul>
<li>Hardware</li>
<li>Parallel processing terms and concepts</li>
<li>Approaches to parallelization
<ul>
<li>Embarrassingly parallel computation</li>
<li>Threaded computations</li>
<li>Multi-process computations</li>
</ul></li>
<li>Considerations in parallelizing your work</li>
</ul></li>
<li>Submitting and monitoring parallel jobs on Savio (Nicolas)
<ul>
<li>Job submission flags</li>
<li>MPI- and openMP- based submission examples</li>
<li>Monitoring jobs to check parallelization</li>
</ul></li>
<li>Parallelization using existing software (Christopher)
<ul>
<li>How to look at documentation to understand parallel capabilities</li>
<li>Specific examples</li>
</ul></li>
<li>Embarrassingly parallel computation (Wei)
<ul>
<li>GNU parallel</li>
<li>Job submission details</li>
</ul></li>
<li>Parallelization in Python, R, and MATLAB (Chris; time permitting)
<ul>
<li>Dask and ipyparallel in Python</li>
<li>future and other packages in R</li>
<li>parfor in MATLAB</li>
</ul></li>
</ul>
</div>
<div id="introduction-savio" class="slide section level1">
<h1>Introduction: Savio</h1>
<p>Savio is a cluster of hundreds of computers (aka ‘nodes’), each with many CPUs (cores), networked together.</p>
<center>
<img src="figures/savio_diagram.jpeg">
</center>
</div>
<div id="introduction-multi-core-computers" class="slide section level1">
<h1>Introduction: multi-core computers</h1>
<p>Each computer has its own memory and multiple (12-56) cores per machine.</p>
<center>
<img src="figures/generic_machine.jpeg">
</center>
<p>savio2 nodes: two Intel Xeon 12-core Haswell processors (24 cores per node (a few have 28))</p>
<p>So a cartoon representation of a cluster like Savio is like <a href="https://waterprogramming.wordpress.com/2017/07/21/developing-parallelised-code-with-mpi-for-dummies-in-c-part-12/">this</a>.</p>
</div>
<div id="introduction-terms-and-concepts" class="slide section level1">
<h1>Introduction: Terms and concepts</h1>
<ul>
<li>Hardware terms
<ul>
<li><strong>cores</strong>: We’ll use this term to mean the different processing units available on a single node. All the cores share main memory.
<ul>
<li><strong>CPUs</strong> and <strong>processors</strong>: These generally have multiple cores, but informally we’ll treat ‘core’, ‘cpu’, and ‘processor’ as being equivalent.</li>
<li><strong>hardware threads</strong> / <strong>hyperthreading</strong>: on some processors, each core can have multiple hardware threads, which are sometimes (but not on Savio) viewed as separate ‘cores’</li>
</ul></li>
<li><strong>nodes</strong>: We’ll use this term to mean the different machines (computers), each with their own distinct memory, that make up a cluster or supercomputer.</li>
</ul></li>
<li>Process terms
<ul>
<li><strong>processes</strong>: individual running instances of a program.
<ul>
<li>seen as separate lines in <code>top</code> and <code>ps</code></li>
</ul></li>
<li><strong>software threads</strong>: multiple paths of execution within a single process; the OS sees the threads as a single process, but one can think of them as ‘lightweight’ processes.
<ul>
<li>seen as &gt;100% CPU usage in <code>top</code> and <code>ps</code></li>
</ul></li>
<li><strong>workers</strong>: the individual processes that are carrying out a (parallelized) computation (e.g., Python, R, or MATLAB workers controlled from the master Python/R/MATLAB process).</li>
<li><strong>(computational) tasks</strong>: individual computations needing to be done
<ul>
<li>easily confused with <strong>MPI tasks</strong>: the individual processes run as part of an MPI computation.</li>
<li>easily confused with <strong>SLURM tasks</strong>: the number of individual processes you plan to run.</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="introduction-high-level-considerations" class="slide section level1">
<h1>Introduction: High-level considerations</h1>
<p>Parallelization:</p>
<ul>
<li>Ideally we have no more (often the same number of) processes or processes+threads than the cores on a node.</li>
<li>We generally want at least as many computational tasks as cores available to us.</li>
</ul>
<p>Speed:</p>
<ul>
<li>Getting data from the CPU cache for processing is fast.</li>
<li>Getting data from main memory (RAM) is slower.</li>
<li>Moving data across the network (e.g., between nodes) is much slower, as is reading data off disk.
<ul>
<li>Infiniband networking between compute nodes and to /global/scratch is much faster than Ethernet networking to login nodes and to /global/home/users</li>
</ul></li>
<li>Moving data over the internet is even slower.</li>
</ul>
</div>
<div id="introduction-common-bottlenecks" class="slide section level1">
<h1>Introduction: Common Bottlenecks</h1>
<ul>
<li>Constrained by memory (RAM) bandwidth
<ul>
<li>For example, each thread is loading lots of data into memory</li>
</ul></li>
<li>Constrained by filesystem bandwidth
<ul>
<li>For example, 10 nodes all trying to read the same large file on scratch</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s Law</a>: Your task must take at least as long as the sequential part.
<ul>
<li>For example, if every task has to load a Python library that takes 10 seconds, then your job will take at least 10 seconds. If not all the tasks are running at the same time, then it will take much more than that.</li>
</ul></li>
</ul>
<p>Possible solutions:</p>
<ul>
<li>Fewer tasks per node will reduce strain on memory
<ul>
<li>You would still be given exclusive access to the node (and charged for that), so there will be a tradeoff here with the cost. It’s possible running fewer tasks will give you better performance so you take less time and save money, but you are also running fewer tasks at a time. The sweet spot will depend on your particular task.</li>
</ul></li>
<li>Reduce number of filesystem operations</li>
<li>Find ways to reduce sequential bottlenecks</li>
</ul>
</div>
<div id="introduction-types-of-parallelization" class="slide section level1">
<h1>Introduction: Types of parallelization</h1>
<ul>
<li>Embarrassingly parallel computation of multiple jobs on one or more nodes*</li>
<li>Parallelize one job over CPU cores*</li>
<li>Parallelize one job over multiple nodes*</li>
<li>Vector instructions (AVX2/AVX512) allow vectorized arithmetic on recent Intel CPUs (e.g., savio2, savio3 nodes)</li>
</ul>
<p>* focusing on these strategies today</p>
</div>
<div id="types-of-parallelization-embarrassingly-parallel-computation" class="slide section level1">
<h1>Types of parallelization: Embarrassingly parallel computation</h1>
<p>Aka: naturally, pleasingly, perfectly parallel</p>
<p>You have a certain number of computational tasks to carry out and there are no dependencies between the tasks.</p>
<ul>
<li>Naturally parallel</li>
<li>Pleasingly parallel</li>
<li>Perfectly parallel</li>
</ul>
<p>E.g., process multiple datasets, or do a parameter sweep over multiple parameter sets.</p>
<p>Tools:</p>
<ul>
<li><a href="https://www.gnu.org/software/parallel/parallel_tutorial.html#GNU-Parallel-Tutorial">GNU parallel</a> (standard Linux tool)*</li>
<li><a href="https://research-it.berkeley.edu/services/high-performance-computing/user-guide/hthelper-script">ht_helper (Savio-specific tool)</a></li>
<li>SLURM job arrays (careful, one job per node…)</li>
</ul>
<p>* focusing on this approach today</p>
</div>
<div id="types-of-parallelization-threaded-computations" class="slide section level1">
<h1>Types of parallelization: Threaded computations</h1>
<ul>
<li>Single process controls execution</li>
<li>Use of code libraries that allow the process to split a computation (see <a href="demos/openmp_example.c">this OpenMP example</a> across multiple software threads (still one process)</li>
</ul>
<pre><code>  PID  USER  PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                  
15876 zhang  20   0 17.321g 0.015t  91828 S 410.6  5.9  14158:58 python                                                   </code></pre>
<ul>
<li>Threads share memory and data structures (beware ‘race’ conditions)</li>
<li>Single node ONLY!</li>
</ul>
<center>
<img src="figures/threads.gif">
</center>
<p>(Image provided by <a href="https://computing.llnl.gov/tutorials/openMP">https://computing.llnl.gov/tutorials/openMP</a>.)</p>
</div>
<div id="types-of-parallelization-threaded-computations-part-2" class="slide section level1">
<h1>Types of parallelization: Threaded computations (part 2)</h1>
<p>Examples in standard software:</p>
<ul>
<li>MATLAB threads some computations behind the scenes
<ul>
<li>vectorized calculations</li>
<li>linear algebra (MKL)</li>
</ul></li>
<li>If set up appropriately (true on Savio), R and Python rely on threaded linear algebra (OpenBLAS)</li>
</ul>
<p>Rolling your own:</p>
<ul>
<li>OpenMP (for C/C++/Fortran)</li>
<li>pthreads (for C/C++)</li>
<li>Intel threaded building blocks (TBB) (for C++)</li>
</ul>
</div>
<div id="types-of-parallelization-multi-process-computations" class="slide section level1">
<h1>Types of parallelization: Multi-process computations</h1>
<p>Standard software (e.g., Python, R, MATLAB) allow you to start up multiple workers and farm out independent computations.</p>
<ul>
<li>Master process controls execution</li>
<li>Workers are separate processes</li>
</ul>
<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                  
21718 lin-er    20   0 1171176 930020   4704 R 100.0  0.7   1:18.90 R                                                        
21729 lin-er    20   0 1173352 931888   4524 R 100.0  0.7   1:05.89 R                                                        
21714 lin-er    20   0 1189080 947924   4704 R 100.0  0.7   1:18.95 R                                                        
21715 lin-er    20   0 1151192 909984   4704 R 100.0  0.7   1:18.97 R                                                        
21716 lin-er    20   0 1174212 933056   4704 R 100.0  0.7   1:18.93 R                                                        </code></pre>
<ul>
<li>Often have more computational tasks than workers</li>
<li>Examples:
<ul>
<li>Python: Dask, ipyparallel, ray</li>
<li>R: future, foreach, parallel::parLapply</li>
<li>MATLAB: parfor</li>
</ul></li>
<li>Easy to run on one node with limited (1-2 lines of code) setup</li>
<li>Many of these can run across multiple nodes but require <strong>user</strong> to set things up (see examples in last section of this material).</li>
</ul>
</div>
<div id="types-of-parallelization-distributed-computations" class="slide section level1">
<h1>Types of parallelization: Distributed computations</h1>
<p>Traditional high-performance computing (HPC) runs a large computation by (carefully) splitting the computation amongst communicating MPI workers (MPI ‘tasks’ or ‘ranks’).</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1"></a>$ <span class="ex">mpicc</span> mpi_example.c -o mpi_example</span>
<span id="cb3-2"><a href="#cb3-2"></a>$ <span class="ex">mpirun</span> -np 20 mpi_example</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co"># Hello from MPI process 2 out of 20 on savio2.n0070</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co"># Hello from MPI process 0 out of 20 on savio2.n0096</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co"># Hello from MPI process 18 out of 20 on savio2.n0070</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co"># Hello from MPI process 7 out of 20 on savio2.n0098</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co"># &lt;snip&gt;</span></span></code></pre></div>
<p>Comments:</p>
<ul>
<li>multiple (e.g., 20 above) copies of the same executable are run at once (SPMD)</li>
<li>code behaves differently based on the ID (‘rank’) of the process</li>
<li>processes communicate by sending ‘messages’</li>
<li>MPI can be used on a single node if desired.</li>
<li>OpenMPI is the standard MPI library but there are others</li>
<li>Can run threaded code within MPI processes</li>
</ul>
</div>
<div id="introduction-other-kinds-of-parallel-computing" class="slide section level1">
<h1>Introduction: Other kinds of parallel computing</h1>
<ul>
<li>GPU computation
<ul>
<li>Thousands of slow cores</li>
<li>Groups of cores do same computation at same time in lock-step</li>
<li>Separate GPU memory</li>
<li>Users generally don’t code for a GPU directly but use packages such as Tensorflow or PyTorch</li>
<li>Multi-GPU computation has some commonalities with multi-node MPI work</li>
</ul></li>
<li>Spark/Hadoop
<ul>
<li>Data distributed across disks of multiple machines</li>
<li>Each processor works on data local to the machine</li>
<li>Spark tries to keep data in memory</li>
</ul></li>
</ul>
</div>
<div id="parallel-processing-considerations" class="slide section level1">
<h1>Parallel processing considerations</h1>
<ul>
<li>Use all the cores on a node fully.
<ul>
<li>Have as many worker processes as all the cores available.</li>
<li>Have at least as many computational tasks as processes (often many more).</li>
</ul></li>
<li>Use multiple nodes if you need more cores or more (total) memory.</li>
<li>Starting up worker processes and sending data involves a delay (latency).
<ul>
<li>Don’t have very many tasks that each run very quickly.</li>
<li>Don’t send many small chunks of data.</li>
</ul></li>
<li>Having tasks with highly variable completion times can lead to poor load-balancing (particularly with relatively few tasks).</li>
<li>Writing code for computations with dependencies is much harder than embarrassingly parallel computation.</li>
</ul>
</div>
<div id="submitting-and-monitoring-jobs" class="slide section level1">
<h1>Submitting and Monitoring Jobs</h1>
</div>
<div id="submitting-jobs-available-hardware" class="slide section level1">
<h1>Submitting jobs: Available Hardware</h1>
<ul>
<li>Partitions
<ul>
<li>savio (Intel Xeon E5-2670 v2): 164 nodes, 20 CPU cores each (Supports AVX2 instructions)</li>
<li>savio2 (Intel Xeon E5-2670 v3): 136 nodes, 24 CPU cores each (Supports AVX2 instructions)</li>
<li>and others…</li>
</ul></li>
<li>See the <a href="https://research-it.berkeley.edu/services/high-performance-computing/user-guide/savio-user-guide#Hardware">Savio User Guide</a> for more details.</li>
</ul>
</div>
<div id="submitting-jobs-slurm-scheduler" class="slide section level1">
<h1>Submitting jobs: Slurm Scheduler</h1>
<p>Slurm is a job scheduler that allows you to request resources and queue your job to run when available.</p>
<h2 id="slurm-environment-variables-for-parellelism">Slurm Environment Variables (for parellelism)</h2>
<p>Slurm provides various environment variables that your program can read that may be useful for managing how to distribute tasks. These are taken from the full list of Slurm environment variables listed on the <a href="https://slurm.schedmd.com/sbatch.html">Slurm sbatch documentation</a>.</p>
<ul>
<li><code>$SLURM_NNODES</code> - Total number of nodes in the job’s resource allocation.</li>
<li><code>$SLURM_NODELIST</code> - List of nodes allocated to the job.</li>
<li><code>$SLURM_JOB_CPUS_PER_NODE</code> - Count of processors available to the job on this node.</li>
<li><code>$SLURM_CPUS_PER_TASK</code> - Number of cpus requested per task.</li>
</ul>
</div>
<div id="submitting-parallel-jobs" class="slide section level1">
<h1>Submitting Parallel Jobs</h1>
<p>Examples taken from: <a href="https://research-it.berkeley.edu/services/high-performance-computing/running-your-jobs">Running your Jobs</a></p>
<h2 id="openmp-job">OpenMP Job</h2>
<p>Notice that we set <code>--cpus-per-task</code> and then access the environment variable to set the OMP number of threads to use. If you have another way of doing multithreaded tasks, you can use the same <code>$SLURM_CPUS_PER_TASK</code> environment variable.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="co"># Job name:</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="co">#SBATCH --job-name=test</span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="co">#</span></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co"># Account:</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="co">#SBATCH --account=account_name</span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co">#</span></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co"># Partition:</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="co">#SBATCH --partition=partition_name</span></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="co">#</span></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># Request one node:</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb4-13"><a href="#cb4-13"></a><span class="co">#</span></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co"># Specify one task:</span></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="co">#SBATCH --ntasks-per-node=1</span></span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="co">#</span></span>
<span id="cb4-17"><a href="#cb4-17"></a><span class="co"># Number of processors for single task needed for use case (example):</span></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co">#SBATCH --cpus-per-task=4</span></span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="co">#</span></span>
<span id="cb4-20"><a href="#cb4-20"></a><span class="co"># Wall clock limit:</span></span>
<span id="cb4-21"><a href="#cb4-21"></a><span class="co">#SBATCH --time=00:00:30</span></span>
<span id="cb4-22"><a href="#cb4-22"></a><span class="co">#</span></span>
<span id="cb4-23"><a href="#cb4-23"></a><span class="co">## Command(s) to run (example):</span></span>
<span id="cb4-24"><a href="#cb4-24"></a><span class="bu">export</span> <span class="va">OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK</span></span>
<span id="cb4-25"><a href="#cb4-25"></a><span class="ex">./a.out</span></span></code></pre></div>
<h2 id="openmpi-job">OpenMPI job</h2>
<p>Notice we request 2 nodes and have 20 tasks per node. We could also leave out the number of nodes requested and simply say 40 tasks, and then Slurm would know we need 2 nodes since we said to use 1 CPU per task.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">#!/bin/bash</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co"># Job name:</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co">#SBATCH --job-name=test</span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co">#</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co"># Account:</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="co">#SBATCH --account=account_name</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co">#</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co"># Partition:</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co">#SBATCH --partition=partition_name</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co">#</span></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="co"># Number of nodes needed for use case:</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co">#SBATCH --nodes=2</span></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="co">#</span></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co"># Tasks per node based on number of cores per node (example):</span></span>
<span id="cb5-15"><a href="#cb5-15"></a><span class="co">#SBATCH --ntasks-per-node=20</span></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co">#</span></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="co"># Processors per task:</span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="co">#SBATCH --cpus-per-task=1</span></span>
<span id="cb5-19"><a href="#cb5-19"></a><span class="co">#</span></span>
<span id="cb5-20"><a href="#cb5-20"></a><span class="co"># Wall clock limit:</span></span>
<span id="cb5-21"><a href="#cb5-21"></a><span class="co">#SBATCH --time=00:00:30</span></span>
<span id="cb5-22"><a href="#cb5-22"></a><span class="co">#</span></span>
<span id="cb5-23"><a href="#cb5-23"></a><span class="co">## Command(s) to run (example):</span></span>
<span id="cb5-24"><a href="#cb5-24"></a><span class="ex">module</span> load gcc openmpi</span>
<span id="cb5-25"><a href="#cb5-25"></a><span class="ex">mpirun</span> ./a.out</span></code></pre></div>
</div>
<div id="monitoring-jobs" class="slide section level1">
<h1>Monitoring Jobs</h1>
<p>How do I know if my job is using resources efficiently?</p>
<h3 id="while-job-is-running">While job is running</h3>
<p>Using <code>srun</code> and <code>htop</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1"></a><span class="ex">srun</span> --jobid=<span class="va">$JOB_ID</span> --pty bash -i</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">uptime</span> <span class="co"># check the load</span></span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># use htop for a visual representation of CPU usage</span></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="ex">module</span> load htop</span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="ex">htop</span></span></code></pre></div>
<p>Using warewulf (might not be working):</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1"></a><span class="ex">wwall</span> -j <span class="va">$JOB_ID</span></span></code></pre></div>
<p>Using <a href="http://metacluster.lbl.gov/warewulf">metacluster</a> (imprecise but can give general idea if you know which nodes you’re on):</p>
<h3 id="after-the-job-has-run">After the job has run</h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1"></a><span class="ex">sacct</span> -j <span class="va">$JOB_ID</span> --format JobID,TotalCPU,CPUTime,NCPUs,MaxRSS,Start,End</span></code></pre></div>
<p>Not perfectly accurate, since it measures only the parent process of your job (not child processes). Ideally, <code>TotalCPU</code> will be as close as possible to <code>CPUTime</code>.</p>
</div>
<div id="parallelization-using-existing-software" class="slide section level1">
<h1>Parallelization using existing software</h1>
<p>Many times you simply want to use an existing tool to perform an analysis. Parallelization depends on how the developer designed the software, and ease of use usually depends on how well documented the software is.</p>
<p>There are three main ways in which parallelization parameters are set:</p>
<ul>
<li>command line arguments</li>
<li>compilation options that produce separate executables</li>
<li>configuration files</li>
</ul>
</div>
<div id="command-line-arguments" class="slide section level1">
<h1>Command line arguments</h1>
<p>Usually command line tools have arguments to set the level of parallelization. e.g.</p>
<ul>
<li><code>-t</code> “threads”</li>
<li><code>-p</code> “processes”</li>
<li><code>-n</code> “number of processes/threads”</li>
<li><code>-c</code> “cores”</li>
</ul>
</div>
<div id="bowtie2" class="slide section level1">
<h1>Bowtie2</h1>
<p>Bowtie2 is a popular read aligner used in bioinformatics. Next generation sequencing tools often produce millions or billions of short sequences, or “reads”. Figuring out which part of the genome these reads came from is a common task called “alignment”. This task is highly parallelizable because (once an index is made) aligning each read can be done independently.</p>
<p>From the Manual, under Building from source:</p>
<blockquote>
<p>By default, Bowtie 2 uses the Threading Building Blocks library (TBB) for this. [If TBB is not available] Bowtie 2 can be built with make NO_TBB=1 to use pthreads or Windows native multithreading instead.</p>
</blockquote>
<p>From the manual, under Performance Options:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1"></a><span class="ex">-p/--threads</span> NTHREADS</span></code></pre></div>
<blockquote>
<p>Launch NTHREADS parallel search threads (default: 1). Threads will run on separate processors/cores and synchronize when parsing reads and outputting alignments. Searching for alignments is highly parallel, and speedup is close to linear. Increasing -p increases Bowtie 2’s memory footprint. E.g. when aligning to a human genome index, increasing -p from 1 to 8 increases the memory footprint by a few hundred megabytes. This option is only available if bowtie is linked with the pthreads library (i.e. if BOWTIE_PTHREADS=0 is not specified at build time).</p>
</blockquote>
</div>
<div id="bowtie2-uses-multithreading" class="slide section level1">
<h1>Bowtie2 uses multithreading:</h1>
<ul>
<li>Can use all the cores on a node easily</li>
<li>Can not be run across multiple nodes</li>
<li>Further parallelization will require splitting up the inputs &amp; merging results after</li>
</ul>
</div>
<div id="blast" class="slide section level1">
<h1>BLAST</h1>
<p>The Basic Local Alignment Search Tool (BLAST) is another very common alignment tool designed not to find which location in a genome a short sequence comes from but to identify the origin of a larger sequence from a database of all known sequences.</p>
<p>BLASTing many queries</p>
<ul>
<li>BLAST takes input files with multiple queries</li>
<li>Some speedup is achieved by batch searching - multiple queries are scanned against the DB at once</li>
<li>Batching can increase memory use</li>
</ul>
<p>Buried in the appendices of the BLAST documentation is this option:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1"></a><span class="ex">-num_threads</span></span></code></pre></div>
<blockquote>
<p>Number of threads (CPUs) to use in blast search.</p>
</blockquote>
<p>The implementation here is not well documented, but it’s safe to assume this means threading with shared memory, and thus won’t work across nodes. Still, a few quick tests shows that <a href="https://voorloopnul.com/blog/how-to-correctly-speed-up-blast-using-num_threads/">the <code>-num_threads</code> option results in a non-linear speedup that quickly levels off around -num_threads 4.</a></p>
</div>
<div id="blast-is-an-excellent-candidate-for-gnu-parallel-or-ht_helper." class="slide section level1">
<h1>BLAST is an excellent candidate for GNU parallel or ht_helper.</h1>
<ul>
<li>covered in the next section</li>
<li>splitting your queries and running multiple instances of BLAST is faster than the -num_threads option</li>
</ul>
</div>
<div id="gatk" class="slide section level1">
<h1>GATK</h1>
<p>The Genome Analysis Tool Kit is a tool for making statistical inferences about genotypes or gene frequencies from sets of alignments like those produced by Bowtie2. The GATK manual is very large (many different tools), and parralelization documentation doesn’t jump out. Fortunately, GATK has a <a href="https://gatkforums.broadinstitute.org/gatk/discussion/1975/how-can-i-use-parallelism-to-make-gatk-tools-run-faster">large helpful community producing guides.</a></p>
<p>GATK uses different terminology</p>
<ul>
<li>Machines vs Nodes</li>
<li>“data threads” and “cpu threads”</li>
<li>scatter-gather (general technique) to describe multi-node processing (paradigm)</li>
</ul>
<p>The key options, usable in some tools, are</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1"></a><span class="ex">-nt</span> / --num_threads</span></code></pre></div>
<blockquote>
<p>controls the number of data threads sent to the processor</p>
</blockquote>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1"></a><span class="ex">-nct</span> / --num_cpu_threads_per_data_thread</span></code></pre></div>
<blockquote>
<p>controls the number of CPU threads allocated to each data thread</p>
</blockquote>
<p>Reading the guides, <code>-nt</code> duplicates the data in memory because <code>-nt</code> “threads” cannot share memory, while <code>-nct</code> threads use shared memory. This paradigm is sometimes refered to as processes/threads, rather than data-threads/cpu-threads.</p>
</div>
<div id="gatk-has-multiple-levels-of-parallelization" class="slide section level1">
<h1>GATK has multiple levels of parallelization</h1>
<ul>
<li>Different tools</li>
<li>Different resource needs</li>
<li>Different recommended configurations</li>
</ul>
<p>NCT = Threads (Shared memory)<br />
NT = “data threads” (Indpendent memory)<br />
SG = “scatter gather” (Multiple nodes)*</p>
<table class="table table-striped table-bordered table-condensed">
<thead>
<tr>
<th align="left">
Tool
</th>
<th align="left">
Full name
</th>
<th align="left">
Type of traversal
</th>
<th align="center">
NT
</th>
<th align="center">
NCT
</th>
<th align="center">
SG
</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">
RTC
</td>
<td align="left">
RealignerTargetCreator
</td>
<td align="left">
RodWalker
</td>
<td align="center">
+
</td>
<td align="center">
-
</td>
<td align="center">
-
</td>
</tr>
<tr>
<td align="left">
IR
</td>
<td align="left">
IndelRealigner
</td>
<td align="left">
ReadWalker
</td>
<td align="center">
-
</td>
<td align="center">
-
</td>
<td align="center">
+
</td>
</tr>
<tr>
<td align="left">
BR
</td>
<td align="left">
BaseRecalibrator
</td>
<td align="left">
LocusWalker
</td>
<td align="center">
-
</td>
<td align="center">
+
</td>
<td align="center">
+
</td>
</tr>
<tr>
<td align="left">
PR
</td>
<td align="left">
PrintReads
</td>
<td align="left">
ReadWalker
</td>
<td align="center">
-
</td>
<td align="center">
+
</td>
<td align="center">
-
</td>
</tr>
<tr>
<td align="left">
RR
</td>
<td align="left">
ReduceReads
</td>
<td align="left">
ReadWalker
</td>
<td align="center">
-
</td>
<td align="center">
-
</td>
<td align="center">
+
</td>
</tr>
<tr>
<td align="left">
HC
</td>
<td align="left">
HaplotypeCaller
</td>
<td align="left">
ActiveRegionWalker
</td>
<td align="center">
-
</td>
<td align="center">
(+)
</td>
<td align="center">
+
</td>
</tr>
<tr>
<td align="left">
UG
</td>
<td align="left">
UnifiedGenotyper
</td>
<td align="left">
LocusWalker
</td>
<td align="center">
+
</td>
<td align="center">
+
</td>
<td align="center">
+
</td>
</tr>
</tbody>
</table>
</div>
<div id="scatter-gather-in-gatk" class="slide section level1">
<h1>Scatter Gather in GATK</h1>
<ul>
<li>GATK supports use of WDL (Workflow Description Language) to script multi-node capable processing</li>
<li>this is a scripting language to generate multiple GATK calls</li>
<li>probably easier to use GNU parallel for multi-node parallelism</li>
</ul>
</div>
<div id="compilation-optionsseparate-executables" class="slide section level1">
<h1>Compilation options/Separate executables</h1>
<p>Some software can use either multithreading or MPI.</p>
<ul>
<li>MPI implementation requires additional libraries (e.g. OpenMPI)</li>
<li>this functionality is toggled on/off at compilation</li>
</ul>
</div>
<div id="iq-tree" class="slide section level1">
<h1>IQ-TREE</h1>
<p>IQ-TREE is a phylogenetics package that makes a maximum-likelihood estimation of the evolutionary history of a set of sequences (i.e. an evolutionary tree).</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1"></a><span class="ex">-nt</span></span></code></pre></div>
<blockquote>
<p>Specify the number of CPU cores for the multicore version. A special option <code>-nt</code> AUTO will tell IQ-TREE to automatically determine the best number of cores given the current data and computer.</p>
</blockquote>
<p>Some testing will probably be necessary to learn how memory usage and runtime scale with more threads, but the AUTO parameter is a nice convenience feature implemented by some developers.</p>
<p>There is also an MPI version which must be compiled seperately, creating a second binary “iqtree-mpi”.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1"></a><span class="ex">module</span> load gcc</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="ex">module</span> load openmpi</span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="fu">cmake</span> -DIQTREE_FLAGS=mpi ..</span></code></pre></div>
<p>The multicore version is very easy to use on a single node, but large analyses must use the MPI version.</p>
</div>
<div id="config-files-environment-variables" class="slide section level1">
<h1>Config Files &amp; Environment Variables</h1>
<p>Sometimes the parallelization settings are set in configuration files or in environment variables, rather than as arguments to the execution call.</p>
</div>
<div id="abaqus" class="slide section level1">
<h1>Abaqus</h1>
<p>Abaqus is a popular finite element analysis software. Physicist, engineers, and others use this software to test when their designs will break or light on fire, among other things.</p>
<p><code>abaqus_v6.env</code> is a Python syntax config file that is read when Abaqus runs.</p>
<ul>
<li>cpus</li>
<li>gpus</li>
<li>memory</li>
<li>mp_mode</li>
</ul>
</div>
<div id="embarrassingly-parallel-computation" class="slide section level1">
<h1>Embarrassingly parallel computation</h1>
<h3 id="lets-blast-to-align-protein-sequences">Let’s blast to align protein sequences</h3>
<pre><code>[user@n0002 BRC]$ cat blast.sh 
blastp -query protein1.faa -db db/img_v400_PROT.00 -out protein1.seq
blastp -query protein2.faa -db db/img_v400_PROT.00 -out protein2.seq 
blastp -query protein3.faa -db db/img_v400_PROT.00 -out protein3.seq</code></pre>
<h3 id="parallel-on-one-compute-node">Parallel on one compute node</h3>
<pre><code>blastp -query protein1.faa -db db/img_v400_PROT.00 -out protein1.seq &amp;
blastp -query protein2.faa -db db/img_v400_PROT.00 -out protein2.seq &amp;
blastp -query protein3.faa -db db/img_v400_PROT.00 -out protein3.seq &amp;
wait</code></pre>
<h3 id="what-about-more-than-one-nodes">What about more than one nodes</h3>
<pre><code>ssh -n node1 &quot;blastp -query protein1.faa -db db/img_v400_PROT.00 -out protein1.seq&quot; &amp;
ssh -n node1 &quot;blastp -query protein1.faa -db db/img_v400_PROT.00 -out protein1.seq&quot; &amp;
...
ssh -n node2 &quot;blastp -query protein1.faa -db db/img_v400_PROT.00 -out protein1.seq&quot; &amp;</code></pre>
<h3 id="blast-with-gnu-parallel">Blast with GNU parallel</h3>
<p><code>parallel -a blast.sh</code></p>
</div>
<div id="what-is-gnu-parallel" class="slide section level1">
<h1>What is GNU parallel</h1>
<ul>
<li>A shell tool for executing independent jobs in parallel using one or more computers.</li>
<li>Dynamically distribute the tasks across the targeted cores/nodes.</li>
<li>A job can be a single command or a small script</li>
<li>Applicable applications:
<ul>
<li>Serial (most common)</li>
<li>Multi-threaded (most common)</li>
<li>MPI (not common)</li>
</ul></li>
</ul>
<p>See GNU Parallel <a href="https://www.gnu.org/software/parallel/">User Guide</a> for more details.</p>
</div>
<div id="task-list-from-the-command-line" class="slide section level1">
<h1>Task list from the command line:</h1>
<ul>
<li>parallel [OPTIONS] COMMAND {} ::: arguments<br />
<strong>{}</strong>: parameter holder which is automatically replaced from the tasklist.</li>
</ul>
<pre><code>[user@n0002 BRC]$ parallel echo {} ::: a b c
   a
   b
   c   
   
 [user@n0002 BRC]$ parallel echo {} ::: a b c ::: `seq 3`
   a 1
   b 2
   c 3</code></pre>
</div>
<div id="task-list-in-the-format-of-a-text-file" class="slide section level1">
<h1>Task list in the format of a text file</h1>
<ul>
<li>parallel [OPTIONS] COMMAND {} :::: argfile(s)</li>
<li>parallel –a argfile(s) [OPTIONS] COMMAND {}</li>
</ul>
<p>Generate input files and a task list</p>
<pre><code>[user@n0002 BRC]$ mkdir -p input; for i in `seq 500`; do touch input/test$i.in ; echo &quot;test $i&quot; &gt; input/test$i.in ; done
[user@n0002 BRC]$ ls input |tr &#39;\ &#39; &#39;\n&#39; |awk &#39;{print &quot;input/&quot;$0}&#39; &gt;task.lst
[user@n0002 BRC]$ cat task.lst
    input/test0.in
    input/test1.in
    input/test2.in
    ...
[user@n0002 BRC]$ parallel echo {} :::: task.lst    
[user@n0002 BRC]$ parallel -a task.lst echo {}
    input/test0.in
    input/test1.in
    input/test2.in
    ...</code></pre>
<p>Task list can have multipe parameters</p>
<pre><code>[user@n0002 BRC]$ cat task2.csv
    in1,20,out1
    in2,30,out2
    ..
    
[user@n0002 BRC]$ parallel --colsep &#39;\,&#39; -a task2.csv echo {}
    in1 20 out1
    in2 30 out2
    ...</code></pre>
</div>
<div id="example-1-same-applicationcommands-fed-with-different-parameter-sets" class="slide section level1">
<h1>Example 1: Same application/commands fed with different parameter sets</h1>
<p><strong>Parameters and Flags</strong></p>
<pre><code>[user@n0002 BRC]$ echo `hostname` &quot;copy input to output &quot;; cp input/xxx.in output/xxx.out</code></pre>
<p>Launch 4 parallel tasks</p>
<pre><code>[user@n0002 BRC]$ module load gnu-parallel/2019.03.22; mkdir -p output
[user@n0002 BRC]$ parallel --jobs 4 -a task.lst &quot;echo `hostname` &quot;copy input to output &quot;; cp $1 $2&quot; {} output/{/.}.out</code></pre>
<ul>
<li>Whereas
<ul>
<li>–jobs/-j: The number of tasks to run concurrently per node</li>
<li>-a: text file as a task list</li>
<li>{}: 1st parameter taking one line from task.lst per task</li>
<li>{/.} remove file path and extension<br />
</li>
<li>output/{/.}.out: 2nd parameter with a new path and file extension</li>
</ul></li>
<li>Oftentimes, commands need more than one line of code.</li>
<li>A script is used as the core unit (bash, python, R, MatLab code…)</li>
</ul>
<pre><code>[user@n0002 BRC]$ cat hostname.sh
    #!/bin/bash
    ##  Example: a script used for GNU parallel
    echo `hostname` &quot;copy input to output &quot;
    cp $1 $2 
                 
[user@n0002 BRC]$ parallel -j 4 -a task.lst sh hostname.sh {} output/{/.}.out</code></pre>
</div>
<div id="more-gnu-parallel-flags" class="slide section level1">
<h1>More GNU Parallel Flags</h1>
<ul>
<li>–dry-run: print the commands to be run</li>
<li>–sshloginfile/-slf sshloginfile: node list</li>
<li>–workdir/-wd: landing workdir on compute nodes, default is $HOME</li>
<li>–joblog: keep track of completed tasks</li>
<li>–resume: resume to run unfinished tasks based on log info</li>
<li>–progress: display job progress</li>
<li>–eta: estimated time to finish</li>
<li>–load: threshold for CPU load, e.g. 80%</li>
<li>–noswap: new jobs won’t be started if a node is under heavy memory load</li>
<li>–memfree: check if there is enough free memory, e.g. 2G</li>
</ul>
<pre><code>[user@n0002 BRC]$ cat hostfile
    n0148.savio3
    n0149.savio3

[user@n0002 BRC]$ parallel --progress --eta --slf hostfile --wd `pwd` --joblog job.log --resume --jobs 4 -a task.lst sh hostname.sh {} output/{/.}.out
...
ETA: 61s Left: 492 AVG: 0.12s  n0148.savio3:4/4/50%/0.5s  n0149.savio3:4/4/50%/0.5s n0148.savio3 copy input to output
ETA: 60s Left: 491 AVG: 0.11s  n0148.savio3:4/5/52%/0.4s  n0149.savio3:4/4/47%/0.5s n0149.savio3 copy input to output
ETA: 60s Left: 490 AVG: 0.10s  n0148.savio3:4/5/52%/0.4s  n0149.savio3:3/5/47%/0.4s n0148.savio3 copy input to output
...</code></pre>
<p>Time Comparision :</p>
<pre><code>[user@n0002 BRC]$ time for i in `cat task.lst`; do out=$(basename $i|sed s/in/out/|awk &#39;{print &quot;output/&quot;$0}&#39;); sh hostname.sh $i $out ; done
...
real    0m8.967s
user    0m1.221s
sys 0m3.222s

[user@n0002 BRC]$ time parallel --jobs 1 -a task.lst  sh hostname.sh {} output/{/.}.out
...
real    0m7.628s
user    0m1.390s
sys 0m3.414s

[user@n0002 BRC]$ time parallel --jobs 4 -a task.lst --joblog j.log  sh hostname.sh {} output/{/.}.out
....
real    0m1.450s
user    0m1.422s
sys 0m3.231s
</code></pre>
</div>
<div id="log" class="slide section level1">
<h1>Log</h1>
<pre><code>[user@n0002 BRC]$ head -3 job.log
Seq     Host    Starttime       JobRuntime      Send    Receive Exitval Signal  Command
1       n0149.savio3    1587243717.218       1.282      0       82      0       0       sh hostname.sh input/test1.in output/test1.out
2       n0148.savio3    1587243717.221       1.279      0       84      0       0       sh hostname.sh input/test10.in output/test10.out
</code></pre>
<p>Typically <em>–log logfile</em> pairs with the <em>resume</em> flag for production runs. Note: a uniq name for logfile is recommended, such as $SLURM_JOB_NAME.log Otherwise, job rerun will not start when the same logfile exists</p>
</div>
<div id="example-2-input-from-a-command-list" class="slide section level1">
<h1>Example 2: input from a command list</h1>
<pre><code>[user@n0002 BRC]$ cat commands.lst 
    echo &quot;host = &quot; &#39;`hostname`&#39;
    sh -c &quot;echo today date = ; date&quot; 
    sh -c &quot;echo today date = ; date&quot; 
    ...
    
[user@n0002 BRC] parallel -j 2 &lt; commands.lst
    host =  n0148.savio3
    today date = Sat Apr 18 14:07:33 PDT 2020
    ...</code></pre>
</div>
<div id="example-3-mpi-applications-using-gnu-parallel" class="slide section level1">
<h1>Example 3: MPI applications using GNU Parallel</h1>
<ul>
<li>Traditional MPI job</li>
</ul>
<pre><code>[user@n0002 BRC]$ mpicc hello_rank.c -o hello_rank
[user@n0002 BRC]$ mpirun -np 2 ./hello_rank 1 
Hello1 from processor n0148.savio3, rank 0 out of 2 processors
Hello1 from processor n0148.savio3, rank 1 out of 2 processors
</code></pre>
<ul>
<li>launch independent MPI tasks in parallel</li>
</ul>
<pre><code>[user@n0002 BRC]$ parallel -j 16 mpirun -np 2 ./hello_rank ::: `seq 40`
Hello 13 from processor n0148.savio3, rank 0 out of 2 processors
Hello 13 from processor n0149.savio3, rank 1 out of 2 processors
Hello 2 from processor n0148.savio3, rank 0 out of 2 processors
Hello 2 from processor n0149.savio3, rank 1 out of 2 processors
Hello 7 from processor n0148.savio3, rank 0 out of 2 processors
Hello 7 from processor n0149.savio3, rank 1 out of 2 processors
...</code></pre>
</div>
<div id="job-submission-sample" class="slide section level1">
<h1>Job submission sample</h1>
<p>Number of nodes to request depends on the the size of your task list. We use 2 nodes as an example.</p>
<pre><code>#!/bin/bash
#SBATCH --job-name=gnu-parallel
#SBATCH --account=account_name
#SBATCH --partition=partition_name
#SBATCH --nodes=2
#SBATCH --time=2:00:00

## Command(s) to run (example):

module load gnu-parallel/2019.03.22
export WORKDIR=/global/scratch/$USER/your/path
cd $WORKDIR

export JOBS_PER_NODE=$(( $SLURM_CPUS_ON_NODE*$SLURM_JOB_NUM_NODES )) 
## when each task is multi-threaded, say NTHREADS=2, then JOBS_PER_NODE should be revised as below
## JOBS_PER_NODE=$(( $SLURM_CPUS_ON_NODE * $SLURM_JOB_NUM_NODES / $NTHREADS ))

echo $SLURM_JOB_NODELIST |sed s/\,/\\n/g &gt; hostfile
## when GNU parallel can&#39;t detect core# of remote nodes, say --progress/--eta, 
## core# should be prepended to hostnames. e.g. 32/n0149.savio3
## echo $SLURM_JOB_NODELIST |sed s/\,/\\n/g |awk -v cores=$SLURM_CPUS_ON_NODE &#39;{print cores&quot;/&quot;$1}&#39;&gt; hostfile

PARALLEL=&quot;parallel --progress -j $JOBS_PER_NODE --slf hostfile --wd $WORKDIR --joblog $SLURM_JOB_NAME.log --resume&quot;
$PARALLEL -a task.lst sh hostname.sh {} output/{/.}.out    </code></pre>
</div>
<div id="summary-and-additional-resources" class="slide section level1">
<h1>Summary and Additional Resources</h1>
<p><strong>Summary</strong> - GNU Parallel is an effective and simple tool to parallel independent jobs - Rich set of options, more to explore - Request interactive node(s) to config optimal GNU parallel options for your applications before submitting jobs</p>
<ul>
<li>GNU Parallel <a href="https://www.gnu.org/software/parallel/man.html">man page</a></li>
<li>GPU Parallel <a href="https://www.gnu.org/software/parallel/parallel_tutorial.html">tutorial</a></li>
</ul>
</div>
<div id="parallelization-in-python-r-and-matlab" class="slide section level1">
<h1>Parallelization in Python, R, and MATLAB</h1>
<ul>
<li><p>Support for threaded computations:</p>
<ul>
<li>Python: threaded linear algebra when linked to a parallel BLAS (e.g., OpenBLAS)</li>
<li>R: threaded linear algebra when linked to a parallel BLAS (e.g., OpenBLAS)</li>
<li>MATLAB: threaded linear algebra via MKL plus automatic threading of various functions</li>
</ul></li>
<li><p>Support for multi-process parallelization</p>
<ul>
<li>Python
<ul>
<li>Parallel map operations via <em>ipyparallel</em>, <em>Dask</em>, other packages</li>
<li>Distributed data structures and calculations via <em>Dask</em></li>
</ul></li>
<li>R
<ul>
<li>Parallel for loops with <em>foreach</em> (via <em>doParallel</em>, <em>doFuture</em>, <em>doMPI</em>, <em>doSNOW</em>)</li>
<li>parallel map operations with <code>future.apply::future_lapply</code>, <code>parallel::parLapply</code>, <code>parallel::mclapply</code></li>
</ul></li>
<li>MATLAB: Parallel for loops via <code>parfor</code></li>
</ul></li>
</ul>
</div>
<div id="dask-and-ipyparallel-in-python" class="slide section level1">
<h1>Dask and ipyparallel in Python</h1>
<p>Single node parallelization:</p>
<table>
<thead>
<tr class="header">
<th>Python package</th>
<th>key functions</th>
<th>details</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dask</td>
<td>futures or <code>map</code></td>
<td>‘processes’ or ‘distributed’ schedulers</td>
</tr>
<tr class="even">
<td>Dask</td>
<td>distributed data structures (arrays, dfs, bags)</td>
<td>various schedulers</td>
</tr>
<tr class="odd">
<td>ipyparallel</td>
<td><code>apply</code> or <code>map</code></td>
<td>start workers via <code>ipcluster</code></td>
</tr>
</tbody>
</table>
<p>Multi-node parallelization:</p>
<table>
<thead>
<tr class="header">
<th>Python package</th>
<th>key functions</th>
<th>details</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dask</td>
<td>futures or <code>map</code></td>
<td>‘distributed’ scheduler, start workers in shell</td>
</tr>
<tr class="even">
<td>Dask</td>
<td>distributed data structures (arrays, dfs, bags)</td>
<td>see above</td>
</tr>
<tr class="odd">
<td>ipyparallel</td>
<td><code>apply</code> or <code>map</code></td>
<td>start workers via <code>ipcontroller</code> + <code>srun ipengine</code></td>
</tr>
</tbody>
</table>
<p>See these materials for details:</p>
<ul>
<li><a href="https://github.com/berkeley-scf/tutorial-dask-future">Dask</a></li>
<li><a href="https://research-it.berkeley.edu/services/high-performance-computing/using-python-savio#Parallel-Processing">ipyparallel</a></li>
</ul>
</div>
<div id="multi-node-parallelization-in-python-via-dask" class="slide section level1">
<h1>Multi-node parallelization in Python via Dask</h1>
<p>Example SLURM script:</p>
<pre><code>#!/bin/bash
#SBATCH --partition=savio2
#SBATCH --account=account_name
#SBATCH --ntasks=48
#SBATCH --time=00:20:00
module load python/3.6
export SCHED=$(hostname)
dask-scheduler&amp;
sleep 30
# Start one worker per SLURM &#39;task&#39; (i.e., core)
srun dask-worker tcp://${SCHED}:8786 &amp;   # might need ${SCHED}.berkeley.edu
sleep 60</code></pre>
<p>Example Python code:</p>
<pre><code> # Connect to the cluster via the scheduler.
import os, dask
from dask.distributed import Client
c = Client(address = os.getenv(&#39;SCHED&#39;) + &#39;:8786&#39;)
n = 100000000
p = 96

 # example use of futures
futures = [dask.delayed(calc_mean)(i, n) for i in range(p)]
results = dask.compute(*futures)

 # example use of map()
inputs = [(i, n) for i in range(p)]
future = c.map(calc_mean_vargs, inputs)
results = c.gather(future)</code></pre>
</div>
<div id="future-and-other-packages-in-r" class="slide section level1">
<h1>future and other packages in R</h1>
<p>Single node parallelization:</p>
<table>
<thead>
<tr class="header">
<th>R package</th>
<th>key functions</th>
<th>details</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>future</td>
<td><code>future.apply::future_lapply</code></td>
<td>‘multiprocess’ plan</td>
</tr>
<tr class="even">
<td>future</td>
<td><code>foreach</code> + <code>doFuture</code></td>
<td>‘multiprocess’ plan</td>
</tr>
<tr class="odd">
<td>foreach</td>
<td><code>foreach</code></td>
<td>use <code>doParallel</code></td>
</tr>
<tr class="even">
<td>parallel</td>
<td><code>parLapply</code></td>
<td>use <code>makeCluster</code></td>
</tr>
<tr class="odd">
<td>parallel</td>
<td><code>mclapply</code></td>
<td>only on Linux / MacOS as uses forking</td>
</tr>
</tbody>
</table>
<p>Multi-node parallelization:</p>
<table>
<thead>
<tr class="header">
<th>R package</th>
<th>key functions</th>
<th>details</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>future</td>
<td><code>future.apply::future_lapply</code></td>
<td>‘cluster’ plan</td>
</tr>
<tr class="even">
<td>future</td>
<td><code>foreach</code> + <code>doFuture</code></td>
<td>‘cluster’ plan</td>
</tr>
<tr class="odd">
<td>foreach</td>
<td><code>foreach</code></td>
<td>use <code>doSNOW</code> or <code>doMPI</code></td>
</tr>
<tr class="even">
<td>parallel</td>
<td><code>parLapply</code></td>
<td>use <code>makeCluster</code> with multiple hosts</td>
</tr>
</tbody>
</table>
<p>See these tutorials for details:</p>
<ul>
<li><a href="https://github.com/berkeley-scf/tutorial-dask-future">future</a></li>
<li><a href="https://github.com/berkeley-scf/tutorial-parallel-basics">single node parallelization</a></li>
<li><a href="https://github.com/berkeley-scf/tutorial-parallel-distributed">multiple node parallelization</a></li>
</ul>
</div>
<div id="multi-node-parallelization-in-r-via-future" class="slide section level1">
<h1>Multi-node parallelization in R via future</h1>
<p>Example SLURM script:</p>
<pre><code>#!/bin/bash
#SBATCH --partition=savio2
#SBATCH --account=account_name
#SBATCH --ntasks=48
#SBATCH --time=00:20:00
module load r
module load r-packages # often useful
R CMD BATCH --no-save run.R run.Rout</code></pre>
<p>Example R code:</p>
<pre><code>library(future)
workers &lt;- system(&#39;srun hostname&#39;, intern = TRUE)
cl &lt;- parallel::makeCluster(workers)
plan(cluster, workers = cl)

 # example use of parallel sapply 
 # This example simply verifies we&#39;re actually connected to the workers.
future_sapply(seq_along(workers), function(i) system(&#39;hostname&#39;, intern = TRUE))

 # example use of foreach
library(doFuture)
results &lt;- foreach(i = 1:200) %dopar% {
   # your code here
}</code></pre>
<p>Note: One can also directly pass the vector of worker names to the <code>workers</code> argument of <code>plan()</code>, which should invoke <code>future::makeClusterPSOCK</code>, but I was having trouble with that hanging on Savio at one point.</p>
</div>
<div id="parfor-in-matlab-on-one-node" class="slide section level1">
<h1>parfor in MATLAB on one node</h1>
<p>Example job script:</p>
<pre><code>#!/bin/bash
#SBATCH --partition=savio2
#SBATCH --account=account_name
#SBATCH --cpus-per-task=24
#SBATCH --time=00:20:00
module load matlab
matlab -nodisplay -nosplash -nodesktop -r run.m</code></pre>
<p>Example MATLAB code:</p>
<pre><code>parpool(str2num(getenv(&#39;SLURM_CPUS_PER_TASK&#39;)));
parfor i = 1:n
 % computational code here
end</code></pre>
</div>
<div id="parfor-in-matlab-on-multiple-nodes" class="slide section level1">
<h1>parfor in MATLAB on multiple nodes</h1>
<ul>
<li>See <a href="https://research-it.berkeley.edu/services/high-performance-computing/using-matlab-savio/running-matlab-jobs-across-multiple-savio">these instructions for MATLAB version 2017b</a>
<ul>
<li>limited to 32 workers (but each worker can do threaded computations)</li>
</ul></li>
<li>For use with MATLAB 2019b, please email us.
<ul>
<li>no limit on the number of workers</li>
</ul></li>
</ul>
</div>
<div id="how-to-get-additional-help" class="slide section level1">
<h1>How to get additional help</h1>
<ul>
<li>For technical issues and questions about using Savio:
<ul>
<li>brc-hpc-help@berkeley.edu</li>
</ul></li>
<li>For questions about computing resources in general, including cloud computing:
<ul>
<li>brc@berkeley.edu</li>
<li>(virtual) office hours: Wed. 1:30-3:00 and Thur. 9:30-11:00</li>
</ul></li>
<li>For questions about data management (including HIPAA-protected data):
<ul>
<li>researchdata@berkeley.edu</li>
<li>(virtual) office hours: Wed. 1:30-3:00 and Thur. 9:30-11:00</li>
</ul></li>
</ul>
<p>Zoom links for virtual office hours: <a href="https://research-it.berkeley.edu/programs/berkeley-research-computing/research-computing-consulting">https://research-it.berkeley.edu/programs/berkeley-research-computing/research-computing-consulting</a></p>
</div>
<div id="upcoming-events-and-hiring" class="slide section level1">
<h1>Upcoming events and hiring</h1>
<ul>
<li>Research IT is hiring graduate students as domain consultants. Please chat with one of us if interested.</li>
</ul>
</div>
</body>
</html>
